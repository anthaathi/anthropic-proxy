spec:
  # Provider configurations
  # Each provider needs a type, endpoint, and API key
  providers:
    # === ANTHROPIC PROVIDERS ===
    # Providers using Anthropic API format (/v1/messages)

    # Anthropic official API
    anthropic:
      type: anthropic  # Optional: defaults to "anthropic" if not specified
      endpoint: https://api.anthropic.com
      apiKey: env.ANTHROPIC_API_KEY

    # OpenRouter - Multi-model API gateway (Anthropic format)
    openrouter:
      type: anthropic
      endpoint: https://openrouter.ai/api/v1
      apiKey: env.OPENROUTER_API_KEY

    # Chutes AI - Claude proxy
    chutes:
      type: anthropic
      endpoint: https://claude.chutes.ai
      apiKey: env.CHUTES_API_KEY

    # Z.ai - AI API provider
    zai:
      type: anthropic
      endpoint: https://api.z.ai/api/anthropic
      apiKey: env.ZAI_API_KEY

    # DeepSeek - Chinese AI provider with Anthropic-compatible API
    deepseek:
      type: anthropic
      endpoint: https://api.deepseek.com/anthropic
      apiKey: env.DEEPSEEK_API_KEY

    # Together AI - Model hosting platform
    together:
      type: anthropic
      endpoint: https://api.together.xyz
      apiKey: env.TOGETHER_API_KEY

    # Custom self-hosted endpoint
    selfhosted:
      type: anthropic
      endpoint: http://localhost:11434/v1
      apiKey: "not-needed"  # For local servers that don't require auth

    # AWS Bedrock proxy (example)
    bedrock:
      type: anthropic
      endpoint: https://bedrock-runtime.us-east-1.amazonaws.com
      apiKey: env.AWS_ACCESS_KEY

    # === OPENAI PROVIDERS ===
    # Providers using OpenAI API format (/v1/chat/completions)
    # The proxy will automatically convert between Anthropic and OpenAI formats

    # OpenAI official API
    openai:
      type: openai
      endpoint: https://api.openai.com
      apiKey: env.OPENAI_API_KEY

    # Azure OpenAI
    azure_openai:
      type: openai
      endpoint: https://your-resource-name.openai.azure.com
      apiKey: env.AZURE_OPENAI_API_KEY

    # OpenRouter with OpenAI format
    openrouter_openai:
      type: openai
      endpoint: https://openrouter.ai/api
      apiKey: env.OPENROUTER_API_KEY

    # Together AI with OpenAI format
    together_openai:
      type: openai
      endpoint: https://api.together.xyz
      apiKey: env.TOGETHER_API_KEY

    # Groq - Ultra-fast inference
    groq:
      type: openai
      endpoint: https://api.groq.com/openai
      apiKey: env.GROQ_API_KEY

    # Perplexity AI
    perplexity:
      type: openai
      endpoint: https://api.perplexity.ai
      apiKey: env.PERPLEXITY_API_KEY

  # Retry configuration (optional)
  # Controls how requests are retried on failure
  retry:
    maxRetries: 3                     # Maximum number of retries per provider (default: 3)
    initialDelay: 100ms               # Initial delay before first retry (default: 100ms)
    maxDelay: 5s                      # Maximum delay between retries (default: 5s)
    backoffMultiplier: 2.0            # Exponential backoff multiplier (default: 2.0)

  # Model configurations
  # Each model maps to a provider and can have an alias
  models:
    # === CLAUDE MODELS (via Anthropic providers) ===

    # Claude Opus models
    - name: "claude-opus-4-20250514"
      context: 200000
      alias: "claude-opus*"
      provider: anthropic
      weight: 5

    - name: "anthropic/claude-opus-4-20250514"
      context: 200000
      alias: "claude-opus*"
      provider: openrouter
      weight: 4

    # Claude Sonnet models
    - name: "claude-sonnet-4-5-20250929"
      context: 200000
      alias: "claude-sonnet*"
      provider: anthropic
      weight: 5

    - name: "moonshotai/Kimi-K2-Thinking"
      context: 256000
      alias: "claude-sonnet*"
      provider: chutes
      weight: 3

    - name: "glm-4.6"
      context: 256000
      alias: "claude-sonnet*"
      provider: zai
      weight: 3

    - name: "deepseek-chat"
      context: 200000
      alias: "claude-sonnet*"
      provider: deepseek
      weight: 2

    # Claude Haiku models (fast, lightweight)
    - name: "claude-haiku-3-5-20250110"
      context: 200000
      alias: "claude-haiku*"
      provider: anthropic
      weight: 5

    - name: "GLM-4.5-Air"
      context: 256000
      alias: "claude-haiku*"
      provider: zai
      weight: 3

    - name: "moonshotai/Kimi-K2-Thinking"
      context: 256000
      alias: "claude-haiku*"
      provider: chutes
      weight: 2

    # Exact model name match (no wildcard)
    - name: "claude-3-opus-20240229"
      context: 200000
      alias: "claude-3-opus-20240229"
      provider: anthropic
      weight: 5

    # === OPENAI MODELS (via OpenAI providers) ===
    # Note: Clients still use Anthropic format, proxy converts automatically

    # GPT-4 models
    - name: "gpt-4o"
      context: 128000
      alias: "gpt-4o*"
      provider: openai
      weight: 5

    - name: "gpt-4-turbo"
      context: 128000
      alias: "gpt-4-turbo*"
      provider: openai
      weight: 4

    # GPT-3.5 models
    - name: "gpt-3.5-turbo"
      context: 16385
      alias: "gpt-3.5*"
      provider: openai
      weight: 3

    # Groq models (ultra-fast)
    - name: "llama-3.3-70b-versatile"
      context: 8192
      alias: "llama-3.3*"
      provider: groq
      weight: 5

    - name: "mixtral-8x7b-32768"
      context: 32768
      alias: "mixtral*"
      provider: groq
      weight: 4

  # API keys for authenticating incoming requests
  # These are the keys clients will use to access your proxy
  apiKeys:
    - "$RANDOM_KEY"                     # Will auto-generate a random key on startup
    - "env.PROXY_API_KEY"               # Read from PROXY_API_KEY environment variable
    - "sk-proxy-example-key-123"        # Static API key example

# Configuration notes:
#
# Environment Variables:
#   - Use "env.VAR_NAME" to read from environment variable
#   - Use "$RANDOM_KEY" to auto-generate a secure random key
#   - Or use static values directly
#
# Provider Types:
#   - "anthropic": Provider uses Anthropic API format (/v1/messages)
#   - "openai": Provider uses OpenAI API format (/v1/chat/completions)
#   - Defaults to "anthropic" if not specified
#   - The proxy automatically converts between formats
#
# Model Aliases:
#   - Use "*" for wildcard matching
#   - "opus*" matches any model name starting with "opus"
#   - "*opus" matches any model name ending with "opus"
#   - "opus" matches exactly "opus" (no wildcard)
#
# Provider Endpoints:
#   - Must be valid HTTP/HTTPS URLs
#   - For Anthropic providers: point to endpoints with /v1/messages support
#   - For OpenAI providers: point to endpoints with /v1/chat/completions support
#   - Can be self-hosted or third-party services
#
# Retry Configuration:
#   - Requests are automatically retried on network errors, rate limits (429), and server errors (5xx)
#   - Each provider is retried up to maxRetries times with exponential backoff
#   - After all retries fail, the next provider in the list is tried
#   - Client errors (4xx except 429) are NOT retried
#
# Routing Logic:
#   1. Models with TPS < 40 are excluded
#   2. Models are sorted by weight (higher first)
#   3. If weights are equal, higher TPS wins
#   4. On failure, retries the same provider with backoff
#   5. After retries exhausted, tries next provider in order
#
# Running the proxy:
#   1. Set environment variables:
#      # Anthropic providers
#      export ANTHROPIC_API_KEY=your-anthropic-key
#      export OPENROUTER_API_KEY=your-openrouter-key
#      export CHUTES_API_KEY=your-chutes-key
#      export ZAI_API_KEY=your-zai-key
#      export DEEPSEEK_API_KEY=your-deepseek-key
#      export TOGETHER_API_KEY=your-together-key
#
#      # OpenAI providers
#      export OPENAI_API_KEY=your-openai-key
#      export AZURE_OPENAI_API_KEY=your-azure-key
#      export GROQ_API_KEY=your-groq-key
#      export PERPLEXITY_API_KEY=your-perplexity-key
#
#      # Proxy configuration
#      export PROXY_API_KEY=your-proxy-key
#      export PORT=8080
#      export LOG_LEVEL=INFO  # Options: DEBUG, INFO, WARN, ERROR (default: INFO)
#
#   2. Start the server:
#      go run main.go
#
#   3. Use the proxy:
#      curl -X POST http://localhost:8080/v1/messages \
#        -H "Authorization: Bearer sk-proxy-example-key-123" \
#        -H "Content-Type: application/json" \
#        -d '{
#          "model": "claude-sonnet-4-5-20250929",
#          "messages": [{"role": "user", "content": "Hello, how are you?"}],
#          "max_tokens": 1024
#        }'
#
# Example with alias matching:
#      curl -X POST http://localhost:8080/v1/messages \
#        -H "Authorization: Bearer sk-proxy-example-key-123" \
#        -H "Content-Type: application/json" \
#        -d '{
#          "model": "claude-opus-latest",
#          "messages": [{"role": "user", "content": "Explain quantum computing"}],
#          "max_tokens": 2048
#        }'
#
# Tips:
#   - Use weights to prioritize certain providers or models
#   - Higher weight = higher priority in routing decisions
#   - Models with weight 0 or negative are treated as weight 1
#   - Use wildcard aliases for flexible model routing
#   - Monitor TPS (tokens per second) for performance-based routing
